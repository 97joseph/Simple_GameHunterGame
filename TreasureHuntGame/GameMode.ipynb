{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MARKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class stores the episodes, all the states that come in between the initial state and the terminal state. \n",
    "# This is later used by the agent for learning by experience, called \"exploration\". \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GameExperience(object):\n",
    "    \n",
    "    # model = neural network model\n",
    "    # max_memory = number of episodes to keep in memory. The oldest episode is deleted to make room for a new episode.\n",
    "    # discount = discount factor; determines the importance of future rewards vs. immediate rewards\n",
    "    \n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "    \n",
    "    # Stores episodes in memory\n",
    "    \n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including pirate cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    # Predicts the next action based on the current environment state        \n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    # Returns input and targets from memory, defaults to data size of 10\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class represents the environment, which includes a maze object defined as a matrix. \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "visited_mark = 0.8  # The visited cells are marked by an 80% gray shade.\n",
    "pirate_mark = 0.5   # The current cell where the pirate is located is marked by a 50% gray shade.\n",
    "\n",
    "# The agent can move in one of four directions.\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "class TreasureMaze(object):\n",
    "\n",
    "    # The maze is a two-dimensional Numpy array of floats between 0.0 and 1.0.\n",
    "    # 1.0 corresponds to a free cell and 0.0 to an occupied cell.\n",
    "    # pirate = (row, col) initial pirate position (defaults to (0,0))\n",
    "\n",
    "    def __init__(self, maze, pirate=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"treasure\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not pirate in self.free_cells:\n",
    "            raise Exception(\"Invalid Pirate Location: must sit on a free cell\")\n",
    "        self.reset(pirate)\n",
    "\n",
    "    # This method resets the pirate's position.\n",
    "    \n",
    "    def reset(self, pirate):\n",
    "        self.pirate = pirate\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = pirate\n",
    "        self.maze[row, col] = pirate_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        # To prevent the game from running excessively long, a minimum reward is defined.\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    # This method updates the state based on agent movement (valid, invalid, or blocked).\n",
    "    \n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = pirate_row, pirate_col, mode = self.state\n",
    "\n",
    "        if self.maze[pirate_row, pirate_col] > 0.0:\n",
    "            self.visited.add((pirate_row, pirate_col))  # marks a visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  \n",
    "            mode = 'invalid' # invalid action, no change in pirate position\n",
    "\n",
    "        # New state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    # This method returns a reward based on the agent movement guidelines.\n",
    "    # The agent will be rewarded with positive or negative points, ranging from -1 to 1, for every movement. \n",
    "    # The highest reward is granted when the agent reaches the treasure cell. \n",
    "    # If the agent hits an occupied cell or attempts to go outside the maze boundary, it will incur the highest penalty. \n",
    "    # A penalty is also applied when the agent tries to revisit a cell, to prevent wandering within free cells. \n",
    "    \n",
    "    def get_reward(self):\n",
    "        pirate_row, pirate_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if pirate_row == nrows-1 and pirate_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (pirate_row, pirate_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    # This method keeps track of the state and total reward based on agent action.\n",
    "\n",
    "    def act(self,  action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    # This method returns the current environment state.\n",
    "    \n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    # To help with visualization, this class includes a draw method to visualize the cells. \n",
    "    # Free cells are marked with white and occupied cells with black. \n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the pirate\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = pirate_mark\n",
    "        return canvas\n",
    "\n",
    "    # This method returns the game status.\n",
    "    \n",
    "    def game_status(self):\n",
    "        # If the agentâ€™s total reward goes below the minimum reward, the game is over.\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        pirate_row, pirate_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # If the agent reaches the treasure cell, the game is won.\n",
    "        if pirate_row == nrows-1 and pirate_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        # Game is not complete yet\n",
    "        return 'not_over'\n",
    "\n",
    "    # This method returns the set of valid actions starting from the current cell.\n",
    "    \n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
